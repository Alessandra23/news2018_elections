<<<<<<< HEAD
pal <- rev(sequential_hcl(palette = "YlGnBu", n = 11))
p <- ggplot(plotall, aes(x = reorder(word, freq, function(n) n), y = freq)) +
geom_col(aes(fill = freq)) +
scale_fill_gradientn(
colors = pal,
name = "Frequência\nda palavra",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Notícias sobre as eleições 2018") +
theme_minimal() +
xlab("Palavras") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
# analyse the sentiment of the words used
bing_lex <- get_sentiments("nrc")
fn_sentiment <- all_words_interesting %>%
left_join(bing_lex)
sentiments <- fn_sentiment %>%
filter(!is.na(sentiment)) %>%
group_by(sentiment) %>%
summarise(n = n())
# plot sentiment
p <- ggplot(sentiments, aes(x = reorder(sentiment, n, function(n) n), y = n)) +
geom_col(aes(fill = n)) +
scale_fill_gradientn(
colors = pal,
name = "Sentimento",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Sentimento") +
theme_minimal() +
xlab("Sentimento") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
p <- ggplot(plotall, aes(x = reorder(word, freq, function(n) n), y = freq)) +
geom_col(aes(fill = freq)) +
scale_fill_gradientn(
colors = pal,
name = "Frequência\nda palavra",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Notícias sobre as eleições 2018") +
theme_minimal() +
xlab("Palavras") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
# analyse the sentiment of the words used
bing_lex <- get_sentiments("nrc")
fn_sentiment <- all_words_interesting %>%
left_join(bing_lex)
sentiments <- fn_sentiment %>%
filter(!is.na(sentiment)) %>%
group_by(sentiment) %>%
summarise(n = n())
# plot sentiment
p <- ggplot(sentiments, aes(x = reorder(sentiment, n, function(n) n), y = n)) +
geom_col(aes(fill = n)) +
scale_fill_gradientn(
colors = pal,
name = "Sentimento",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Sentimento") +
theme_minimal() +
xlab("Sentimento") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
alltxt <- map_df(all_txts, ~ tibble(txt = read_file(.x)) %>%
mutate(filename = basename(.x)))
all_txt <- alltxt[c(3,7,11), 1]
=======
uol <- alltxt[11,1]
# Set the text to lowercase
bbc <- tolower(bbc)
oglobo <- tolower(oglobo)
uol <- tolower(uol)
# Turn into individual sentences
s_vBBC <- get_sentences(bbc)
s_vOGL <- get_sentences(oglobo)
s_vUOL <- get_sentences(uol)
svBBC <- get_sentiment(s_vBBC, method = "nrc", language = "portuguese")
svOGL <- get_sentiment(s_vOGL, method = "nrc", language = "portuguese")
svUOL <- get_sentiment(s_vUOL, method = "nrc", language = "portuguese")
# create rolling average
rBBC <- round(length(svBBC)*.1)
rolledBBC <- zoo::rollmean(svBBC, k = rBBC)
rOGL <- round(length(svOGL)*.1)
rolledOGL <- zoo::rollmean(svOGL, k = rOGL)
rUOL <- round(length(svUOL)*.1)
rolledUOL <- zoo::rollmean(svUOL, k = rUOL)
# rescale to same scale
bbcList <- rescale_x_2(rolledBBC)
OGLList <- rescale_x_2(rolledOGL)
UOLList <- rescale_x_2(rolledUOL)
plot(bbcList$x,
OGLList$z,
type="l",
col="blue",
xlab="Narrative Time",
ylab="Emotional Valence")
View(bbcList)
View(OGLList)
View(UOLList)
?rescale_x_2
View(bbcList)
plot(bbcList$z,
OGLList$z,
type="l",
col="blue",
xlab="Narrative Time",
ylab="Emotional Valence")
lines(OGLList$x, OGLList$z, col="red")
plot(bbcList$x,
OGLList$z,
type="l",
col="blue",
xlab="Narrative Time",
ylab="Emotional Valence")
length(bbcList)
length(bbcList$x)
length(bbcList$y)
min(bbcList$y)
poa_sample <- seq(1, length(bbcList$x), by=round(length(bbcList$x)/100))
bov_sample <- seq(1, length(OGLList$x), by=round(length(OGLList$x)/100))
plot(poa_list$x[poa_sample],
poa_list$z[poa_sample],
type="l",
col="blue",
xlab="Narrative Time (sampled)",
ylab="Emotional Valence"
)
plot(bbcList$x[poa_sample],
bbcList$z[poa_sample],
type="l",
col="blue",
xlab="Narrative Time (sampled)",
ylab="Emotional Valence"
)
lines(OGLList$x[bov_sample], OGLList$z[bov_sample], col="red")
uolSample <- seq(1, length(UOLList$x), by=round(length(UOLList$x)/100))
plot(bbcList$x[poa_sample],
bbcList$z[poa_sample],
type="l",
col="blue",
xlab="Narrative Time (sampled)",
ylab="Emotional Valence"
)
lines(OGLList$x[bov_sample], OGLList$z[bov_sample], col="red")
lines(UOLList$x[bov_sample], UOLList$z[bov_sample], col="black")
# Euclidean
dist(rbind(bbcList$z[bbcSample], OGLList$z[oglSample]))
bbcSample <- seq(1, length(bbcList$x), by=round(length(bbcList$x)/100))
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type="l",
col="blue",
xlab="Narrative Time (sampled)",
ylab="Emotional Valence"
)
lines(OGLList$x[bov_sample], OGLList$z[bov_sample], col="red")
lines(UOLList$x[bov_sample], UOLList$z[bov_sample], col="black")
# Euclidean
dist(rbind(bbcList$z[bbcSample], OGLList$z[oglSample]))
oglSample <- seq(1, length(OGLList$x), by=round(length(OGLList$x)/100))
uolSample <- seq(1, length(UOLList$x), by=round(length(UOLList$x)/100))
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type="l",
col="blue",
xlab="Narrative Time (sampled)",
ylab="Emotional Valence"
)
lines(OGLList$x[oglSample], OGLList$z[oglSample], col="red")
lines(UOLList$x[uolSample], UOLList$z[uolSample], col="black")
# Euclidean
dist(rbind(bbcList$z[bbcSample], OGLList$z[oglSample]))
round(length(bbcList$x)/100)
round(length(OGLList$x)/100)
?seq
bbcSample <- seq(1, length(bbcList$x), by=round(length(bbcList$x)/100), length.out = 100)
bbcSample <- seq(1, length(bbcList$x), by=round(length(bbcList$x)/100), length.out = 10)
bbcSample <- seq(1, length(bbcList$x), length.out = 10)
bbcSample
bbcSample <- seq(1, length(bbcList$x), length.out = 100)
oglSample <- seq(1, length(OGLList$x), length.out = 100)
uolSample <- seq(1, length(UOLList$x), length.out = 100)
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type="l",
col="blue",
xlab="Narrative Time (sampled)",
ylab="Emotional Valence"
)
lines(OGLList$x[oglSample], OGLList$z[oglSample], col="red")
lines(UOLList$x[uolSample], UOLList$z[uolSample], col="black")
# Euclidean
dist(rbind(bbcList$z[bbcSample], OGLList$z[oglSample]))
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type="l",
col="blue",
xlab="Narrative Time (sampled)",
ylab="Emotional Valence"
)
lines(OGLList$x[oglSample], OGLList$z[oglSample], col="red")
# Correlation
cor(cbind(bbcList$z[bbcSample], OGLList$z[oglSample]))
## Read data
all_txts <- list.files(pattern = ".txt$")
all_txts
alltxt <- map_df(all_txts, ~ tibble(txt = read_file(.x)) %>%
mutate(filename = basename(.x)))
bbc <- alltxt[3,1]
oglobo <- alltxt[7,1]
uol <- alltxt[11,1]
# Set the text to lowercase
bbc <- tolower(bbc)
oglobo <- tolower(oglobo)
uol <- tolower(uol)
# Turn into individual sentences
s_vBBC <- get_sentences(bbc)
s_vOGL <- get_sentences(oglobo)
s_vUOL <- get_sentences(uol)
# get sentiments
svBBC <- get_sentiment(s_vBBC, method = "nrc", language = "portuguese")
svOGL <- get_sentiment(s_vOGL, method = "nrc", language = "portuguese")
svUOL <- get_sentiment(s_vUOL, method = "nrc", language = "portuguese")
bbcSample <- seq(1, length(bbcList$x), length.out = 100)
oglSample <- seq(1, length(OGLList$x), length.out = 100)
uolSample <- seq(1, length(UOLList$x), length.out = 100)
UOLList <- rescale_x_2(rolledUOL)
## Read data
all_txts <- list.files(pattern = ".txt$")
all_txts
alltxt <- map_df(all_txts, ~ tibble(txt = read_file(.x)) %>%
mutate(filename = basename(.x)))
bbc <- alltxt[3,1]
oglobo <- alltxt[7,1]
uol <- alltxt[11,1]
# Set the text to lowercase
bbc <- tolower(bbc)
oglobo <- tolower(oglobo)
uol <- tolower(uol)
# Turn into individual sentences
s_vBBC <- get_sentences(bbc)
s_vOGL <- get_sentences(oglobo)
s_vUOL <- get_sentences(uol)
# get sentiments
svBBC <- get_sentiment(s_vBBC, method = "nrc", language = "portuguese")
svOGL <- get_sentiment(s_vOGL, method = "nrc", language = "portuguese")
svUOL <- get_sentiment(s_vUOL, method = "nrc", language = "portuguese")
# create rolling average
rBBC <- round(length(svBBC)*.1)
rolledBBC <- zoo::rollmean(svBBC, k = rBBC)
rOGL <- round(length(svOGL)*.1)
rolledOGL <- zoo::rollmean(svOGL, k = rOGL)
rUOL <- round(length(svUOL)*.1)
rolledUOL <- zoo::rollmean(svUOL, k = rUOL)
# scale
bbcList <- rescale_x_2(rolledBBC)
OGLList <- rescale_x_2(rolledOGL)
UOLList <- rescale_x_2(rolledUOL)
# Compare sample
bbcSample <- seq(1, length(bbcList$x), length.out = 100)
oglSample <- seq(1, length(OGLList$x), length.out = 100)
uolSample <- seq(1, length(UOLList$x), length.out = 100)
styler:::style_active_file()
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type = "l",
col = "blue",
xlab = "Time (sampled)",
ylab = "Sentiment"
)
lines(OGLList$x[oglSample], OGLList$z[oglSample], col = "red")
lines(UOLList$x[uolSample], UOLList$z[uolSample], col = "black")
# Correlation
cor(cbind(bbcList$z[bbcSample], OGLList$z[oglSample]))
# Correlation
cor(cbind(bbcList$z[bbcSample], OGLList$z[oglSample], UOLList$z[uolSample]))
# Correlation
c <- cor(cbind(bbcList$z[bbcSample], OGLList$z[oglSample], UOLList$z[uolSample]))
corrplot::corrplot(c)
?corrplot
corrplot::corrplot(c, method = "square", type = "lower")
corrplot::corrplot(c, method = "color", type = "lower")
corrplot::corrplot(c, method = "circle", type = "lower")
corrplot::corrplot(c, method = "circle", type = "lower", diag = F)
# Correlation
c <- cor(cbind(bbcList$z[bbcSample], OGLList$z[oglSample], UOLList$z[uolSample]))
c
colnames(c) <- rownames(c) <- c("BBC", "oglobo", "uol")
corrplot::corrplot(c, method = "circle", type = "lower", diag = F)
corrplot::corrplot(c, method = "color", type = "lower", diag = F)
corrplot::corrplot.mixed(c)
corrplot::corrplot.mixed(c, lower = 'shade', upper = 'pie')
corrplot::corrplot.mixed(c, lower = 'shade', upper = 'circle')
corrplot::corrplot.mixed(c)
corrplot::corrplot.mixed(c, tl.srt = 45)
corrplot::corrplot.mixed(c)
## Read data
>>>>>>> d06a48078b62111a29031b0a4a9fd81ae19afa7a
all_txts <- list.files(pattern = ".txt$")
all_txts
alltxt <- map_df(all_txts, ~ tibble(txt = read_file(.x)) %>%
mutate(filename = basename(.x)))
all_txt <- alltxt[c(3,7,11), 1]
# Set the text to lowercase
text <- tolower(all_txt)
# Remove mentions, urls, emojis, numbers, punctuations, etc.
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
text <- gsub("[[:punct:]]", " ", text)
# Remove spaces and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
<<<<<<< HEAD
# use portuguese language
stopWords <- stopwords("pt")
# add some more stop words
addWords <- c(
"https", "t.co", "rt", "amp", "to",
"in", "1", "a", "1.98", "100", "é", "2", "skui2ubiss",
"skui2tu7ts", "19", "curtir", "responder", "pra", "pas", "aqui" , "ainda",
"vou", "voc", "tcurtir", "vai", "ser", "silva", "ter", "tkika", "est", "oliveira",
"souza", "tmaria", "lima", "lá", "tá", "todos", "porque", "gomes", "agora", "vão",
"assim", "vcs", "tadriana", "tlucio", "paulo", "santos", "tsou", "aí", "rodrigues"
)
stopWords <- c(stopWords, addWords)
# create list of english and portuguese stop words
my_stop_words <- stop_words %>%
select(-lexicon) %>%
bind_rows(data.frame(word = stopWords))
docs <- Corpus(VectorSource(text))
# get data ready for word cloud
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)
rownames(df) <- NULL
head(df)
# remove stop words from data
all_words_interesting <- df %>%
anti_join(my_stop_words)
all_words_interesting %>% head()
plotall <- all_words_interesting %>%
slice(1:20)
pal <- rev(sequential_hcl(palette = "YlGnBu", n = 11))
p <- ggplot(plotall, aes(x = reorder(word, freq, function(n) n), y = freq)) +
geom_col(aes(fill = freq)) +
scale_fill_gradientn(
colors = pal,
name = "Frequência\nda palavra",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Notícias sobre as eleições 2018") +
theme_minimal() +
xlab("Palavras") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
# analyse the sentiment of the words used
bing_lex <- get_sentiments("nrc")
fn_sentiment <- all_words_interesting %>%
left_join(bing_lex)
sentiments <- fn_sentiment %>%
filter(!is.na(sentiment)) %>%
group_by(sentiment) %>%
summarise(n = n())
# plot sentiment
p <- ggplot(sentiments, aes(x = reorder(sentiment, n, function(n) n), y = n)) +
geom_col(aes(fill = n)) +
scale_fill_gradientn(
colors = pal,
name = "Sentimento",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Sentimento") +
theme_minimal() +
xlab("Sentimento") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
# analyse the sentiment of the words used
bing_lex <- get_sentiments("nrc")
fn_sentiment <- all_words_interesting %>%
left_join(bing_lex)
sentiments <- fn_sentiment %>%
filter(!is.na(sentiment)) %>%
group_by(sentiment) %>%
summarise(n = n())
# plot sentiment
p <- ggplot(sentiments, aes(x = reorder(sentiment, n, function(n) n), y = n)) +
geom_col(aes(fill = n)) +
=======
# set colour palette
pal <- rev(sequential_hcl(palette = "YlGnBu", n = 11))
library(RColorBrewer)
library(colorspace)
# set colour palette
pal <- rev(sequential_hcl(palette = "YlGnBu", n = 11))
# use portuguese language
stopWords <- stopwords("pt")
# add some more stop words
addWords <- c(
"https", "t.co", "rt", "amp", "to",
"in", "1", "a", "1.98", "100", "é", "2", "skui2ubiss",
"skui2tu7ts", "19", "curtir", "responder", "pra", "pas", "aqui" , "ainda",
"vou", "voc", "tcurtir", "vai", "ser", "silva", "ter", "tkika", "est", "oliveira",
"souza", "tmaria", "lima", "lá", "tá", "todos", "porque", "gomes", "agora", "vão",
"assim", "vcs", "tadriana", "tlucio", "paulo", "santos", "tsou", "aí", "rodrigues"
)
stopWords <- c(stopWords, addWords)
# create list of english and portuguese stop words
my_stop_words <- stop_words %>%
select(-lexicon) %>%
bind_rows(data.frame(word = stopWords))
# data manipulation
docs <- Corpus(VectorSource(text))
# get data ready for word cloud
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)
rownames(df) <- NULL
# remove stop words from data
all_words_interesting <- df %>%
anti_join(my_stop_words)
all_words_interesting %>% head()
# get top x words
plotall <- all_words_interesting %>%
slice(1:20)
# Plot --------------------------------------------------------------------
p <- ggplot(plotall, aes(x = reorder(word, freq, function(n) n), y = freq)) +
geom_col(aes(fill = freq)) +
>>>>>>> d06a48078b62111a29031b0a4a9fd81ae19afa7a
scale_fill_gradientn(
colors = pal,
name = "Sentimento",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Sentimento") +
theme_minimal() +
xlab("Sentimento") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
<<<<<<< HEAD
bing_lex
install.packages("lexiconPT")
library(lexiconPT)
get_word_sentiment("temer")
help(lexiconPT)
library(lexiconPT)
help(lexiconPT)
data("oplexicon_v3.0")
data("sentiLex_lem_PT02")
op30 <- oplexicon_v3.0
sent <- sentiLex_lem_PT02
op30
sent
oplexicon_v3.0
get_word_sentiment("nordeste")
get_word_sentiment("bolsonaro")
get_word_sentiment("ignorante")
get_sentiments("nordeste")
?get_sentiments
get_sentiments("nrc")
oplexicon_v3.0
sentiLex_lem_PT02
all_words_interesting %>%
mutate(
temer = str_detect(str_to_lower(message), "nordeste"),
lula = str_detect(str_to_lower(message), "lula"),
pt = str_detect(str_to_lower(message), "pt"),
governo = str_detect(str_to_lower(message), "governo"),
bolsonaro = str_detect(str_to_lower(message), "bolsonaro")
)
all_words_interesting %>%
mutate(
nordeste = str_detect(str_to_lower(message), "nordeste"),
lula = str_detect(str_to_lower(message), "lula"),
pt = str_detect(str_to_lower(message), "pt"),
governo = str_detect(str_to_lower(message), "governo"),
bolsonaro = str_detect(str_to_lower(message), "bolsonaro")
)
sentiLex_lem_PT02
sentiLex_lem_PT02
all_words_interesting %>%
mutate(
nordeste = str_detect(str_to_lower(message), "nordeste"),
lula = str_detect(str_to_lower(message), "lula"),
pt = str_detect(str_to_lower(message), "pt"),
governo = str_detect(str_to_lower(message), "governo"),
bolsonaro = str_detect(str_to_lower(message), "bolsonaro")
)
get_word_sentiment("temer")
get_word_sentiment("lula")
oplexicon_v3.0
tail(op30)
op30
all_txts <- list.files(pattern = ".txt$")
all_txts
alltxt <- map_df(all_txts, ~ tibble(txt = read_file(.x)) %>%
mutate(filename = basename(.x)))
bbc <- alltxt[3,1]
oglobo <- alltxt[7,1]
uol <- alltxt[11,1]
all_txt <- alltxt[c(3,7,11), 1]
# Set the text to lowercase
text <- tolower(all_txt)
# Remove mentions, urls, emojis, numbers, punctuations, etc.
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
#text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
# Remove spaces and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
# use portuguese language
stopWords <- stopwords("pt")
# add some more stop words
addWords <- c(
"https", "t.co", "rt", "amp", "to",
"in", "1", "a", "1.98", "100", "é", "2", "skui2ubiss",
"skui2tu7ts", "19", "curtir", "responder", "pra", "pas", "aqui" , "ainda",
"vou", "voc", "tcurtir", "vai", "ser", "silva", "ter", "tkika", "est", "oliveira",
"souza", "tmaria", "lima", "lá", "tá", "todos", "porque", "gomes", "agora",
"assim", "vcs", "tadriana", "tlucio", "paulo", "santos", "tsou", "aí", "rodrigues"
)
stopWords <- c(stopWords, addWords)
# create list of english and portuguese stop words
my_stop_words <- stop_words %>%
select(-lexicon) %>%
bind_rows(data.frame(word = stopWords))
docs <- Corpus(VectorSource(text))
# get data ready for word cloud
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)
rownames(df) <- NULL
head(df)
# remove stop words from data
all_words_interesting <- df %>%
anti_join(my_stop_words)
all_words_interesting %>% head()
plotall <- all_words_interesting %>%
slice(1:20)
pal <- rev(sequential_hcl(palette = "YlGnBu", n = 11))
p <- ggplot(plotall, aes(x = reorder(word, freq, function(n) n), y = freq)) +
geom_col(aes(fill = freq)) +
=======
# look at dictionary lexicon
lexPort <- get_sentiment_dictionary(dictionary = "nrc", language = "portuguese")
lexPort <- lexPort[,2:4]
fn_sentiment <- all_words_interesting %>%
left_join(lexPort)
sentiments <- fn_sentiment %>%
filter(!is.na(sentiment)) %>%
group_by(sentiment) %>%
summarise(n = n())
# plot sentiment
p <- ggplot(sentiments, aes(x = reorder(sentiment, n, function(n) n), y = n)) +
geom_col(aes(fill = n)) +
>>>>>>> d06a48078b62111a29031b0a4a9fd81ae19afa7a
scale_fill_gradientn(
colors = pal,
limits = c(0,500),
name = "Sentimento",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Sentimento") +
theme_minimal() +
xlab("Sentimento") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
<<<<<<< HEAD
plotall <- all_words_interesting %>%
slice(1:50)
pal <- rev(sequential_hcl(palette = "YlGnBu", n = 11))
p <- ggplot(plotall, aes(x = reorder(word, freq, function(n) n), y = freq)) +
geom_col(aes(fill = freq)) +
scale_fill_gradientn(
colors = pal,
name = "Frequência\nda palavra",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
=======
## Read data
all_txts <- list.files(pattern = ".txt$")
all_txts
alltxt <- map_df(all_txts, ~ tibble(txt = read_file(.x)) %>%
mutate(filename = basename(.x)))
bbc <- alltxt[3, 1]
oglobo <- alltxt[7, 1]
uol <- alltxt[11, 1]
all_txt <- alltxt[c(3, 7, 11), 1]
# Set the text to lowercase
text <- tolower(all_txt)
# Remove mentions, urls, emojis, numbers, punctuations, etc.
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
# Turn into individual sentences
s_v <- get_sentences(text)
# Get sentiment of each sentence
s_v_sentiment <- get_sentiment(s_v, method = "nrc", language = "portuguese")
# plot the sentiment of each sentence over time
time <- c(1:length(s_v_sentiment))
ggplot(mapping = aes(x = time, y = s_v_sentiment)) +
geom_line(col = "grey", alpha = 0.5) +
geom_line(aes(y = rollmean(x = s_v_sentiment, k = 20, na.pad = T), color = "")) +
theme_minimal() +
xlab("Time") +
ylab("Sentiment") +
scale_color_manual(values = c('Average' = 'grey43')) +
labs(color = '')
# Dividing text into 100 chunks and looking at Sentiment of chunks over time
percent_vals <- get_percentage_values(s_v_sentiment, bins = 100)
percentTime <- c(1:length(percent_vals))
ggplot(mapping = aes(x = percentTime, y = percent_vals)) +
geom_line(col = "grey", alpha = 0.5) +
geom_line(aes(y = rollmean(x = percent_vals, k = 20, na.pad = T), color = "")) +
theme_minimal() +
xlab("Time") +
ylab("Sentiment") +
scale_color_manual(values = c('Average' = 'grey43')) +
labs(color = '')
# Using Fourier transform
ft_values <- get_transformed_values(
s_v_sentiment,
low_pass_size = 5,
x_reverse_len = 200,
padding_factor = 2,
scale_vals = TRUE,
scale_range = F
)
ftTime <- c(1:length(ft_values))
ggplot(mapping = aes(x = ftTime, y = ft_values)) +
geom_line(col = "black", alpha = 1) +
theme_minimal() +
xlab("Time") +
ylab("Sentiment")
dct_values <- get_dct_transform(
s_v_sentiment,
low_pass_size = 5,
x_reverse_len = 200,
scale_vals = F,
scale_range = T
>>>>>>> d06a48078b62111a29031b0a4a9fd81ae19afa7a
)
# using cosine tranform instead
dctTime <- c(1:length(dct_values))
ggplot(mapping = aes(x = dctTime, y = dct_values)) +
geom_line(col = "black", alpha = 1) +
theme_minimal() +
<<<<<<< HEAD
xlab("Palavras") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
op30
data("oplexicon_v3.0")
data("sentiLex_lem_PT02")
op30 <- oplexicon_v3.0
sent <- sentiLex_lem_PT02
sent
sent
library(tidytext)
library(tidyverse)
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(tm)
library(colorspace)
library(syuzhet)
=======
xlab("Time") +
ylab("Sentiment")
# combining into one plot
simple_plot(s_v_sentiment)
## Read data
>>>>>>> d06a48078b62111a29031b0a4a9fd81ae19afa7a
all_txts <- list.files(pattern = ".txt$")
all_txts
alltxt <- map_df(all_txts, ~ tibble(txt = read_file(.x)) %>%
mutate(filename = basename(.x)))
bbc <- alltxt[3, 1]
oglobo <- alltxt[7, 1]
uol <- alltxt[11, 1]
# Set the text to lowercase
<<<<<<< HEAD
text <- tolower(all_txt)
# Remove mentions, urls, emojis, numbers, punctuations, etc.
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
#text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
# Remove spaces and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
# use portuguese language
stopWords <- stopwords("pt")
# add some more stop words
addWords <- c(
"https", "t.co", "rt", "amp", "to",
"in", "1", "a", "1.98", "100", "é", "2", "skui2ubiss",
"skui2tu7ts", "19", "curtir", "responder", "pra", "pas", "aqui" , "ainda",
"vou", "voc", "tcurtir", "vai", "ser", "silva", "ter", "tkika", "est", "oliveira",
"souza", "tmaria", "lima", "lá", "tá", "todos", "porque", "gomes", "agora", "vão",
"assim", "vcs", "tadriana", "tlucio", "paulo", "santos", "tsou", "aí", "rodrigues"
)
stopWords <- c(stopWords, addWords)
# create list of english and portuguese stop words
my_stop_words <- stop_words %>%
select(-lexicon) %>%
bind_rows(data.frame(word = stopWords))
docs <- Corpus(VectorSource(text))
# get data ready for word cloud
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)
rownames(df) <- NULL
head(df)
# remove stop words from data
all_words_interesting <- df %>%
anti_join(my_stop_words)
all_words_interesting %>% head()
plotall <- all_words_interesting %>%
slice(1:20)
pal <- rev(sequential_hcl(palette = "YlGnBu", n = 11))
p <- ggplot(plotall, aes(x = reorder(word, freq, function(n) n), y = freq)) +
geom_col(aes(fill = freq)) +
scale_fill_gradientn(
colors = pal,
name = "Frequência\nda palavra",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Notícias sobre as eleições 2018") +
theme_minimal() +
xlab("Palavras") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
# look at dictionary lexicon
lexPort <- get_sentiment_dictionary(dictionary = "nrc", language = "portuguese")
lexPort <- lexPort[,2:4]
fn_sentiment <- all_words_interesting %>%
left_join(lexPort)
sentiments <- fn_sentiment %>%
filter(!is.na(sentiment)) %>%
group_by(sentiment) %>%
summarise(n = n())
# plot sentiment
p <- ggplot(sentiments, aes(x = reorder(sentiment, n, function(n) n), y = n)) +
geom_col(aes(fill = n)) +
scale_fill_gradientn(
colors = pal,
name = "Sentimento",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Sentimento") +
theme_minimal() +
xlab("Sentimento") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p
# using alternate method --------------------------------------------------
# using a slightly different method produces similar results
nrc_lex <- get_nrc_sentiment(char_v = text,
language = "portuguese")
d2 <- data.frame(sentiment = names(nrc_lex), n = t(nrc_lex))
rownames(d2) <- NULL
p1 <- ggplot(d2, aes(x = reorder(sentiment, n, function(n) n), y = n)) +
geom_col(aes(fill = n)) +
scale_fill_gradientn(
colors = pal,
name = "Sentimento",
guide = guide_colorbar(
frame.colour = "black",
frame.linewidth = 1,
ticks.colour = "black",
ticks.linewidth = 1,
)
) +
ggtitle(label = "Sentimento") +
theme_minimal() +
xlab("Sentimento") +
ylab("Frequência") +
coord_flip() +
theme(axis.title.y = element_text(angle = 90, vjust = 0.5))
p1
d2
=======
bbc <- tolower(bbc)
oglobo <- tolower(oglobo)
uol <- tolower(uol)
# Turn into individual sentences
s_vBBC <- get_sentences(bbc)
s_vOGL <- get_sentences(oglobo)
s_vUOL <- get_sentences(uol)
# get sentiments
svBBC <- get_sentiment(s_vBBC, method = "nrc", language = "portuguese")
svOGL <- get_sentiment(s_vOGL, method = "nrc", language = "portuguese")
svUOL <- get_sentiment(s_vUOL, method = "nrc", language = "portuguese")
# create rolling average
rBBC <- round(length(svBBC) * .1)
rolledBBC <- zoo::rollmean(svBBC, k = rBBC)
rOGL <- round(length(svOGL) * .1)
rolledOGL <- zoo::rollmean(svOGL, k = rOGL)
rUOL <- round(length(svUOL) * .1)
rolledUOL <- zoo::rollmean(svUOL, k = rUOL)
# scale
bbcList <- rescale_x_2(rolledBBC)
OGLList <- rescale_x_2(rolledOGL)
UOLList <- rescale_x_2(rolledUOL)
# Compare sample
bbcSample <- seq(1, length(bbcList$x), length.out = 100)
oglSample <- seq(1, length(OGLList$x), length.out = 100)
uolSample <- seq(1, length(UOLList$x), length.out = 100)
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type = "l",
col = "blue",
xlab = "Time (sampled)",
ylab = "Sentiment"
)
legend(1, 95, legend=c("Line 1", "Line 2"),
col=c("red", "blue"), lty=1:2, cex=0.8,
title="Line types", text.font=4, bg='lightblue')
legend(1, 1, legend=c("Line 1", "Line 2"),
col=c("red", "blue"), lty=1:2, cex=0.8,
title="Line types", text.font=4, bg='lightblue')
legend(0, 1, legend=c("Line 1", "Line 2"),
col=c("red", "blue"), lty=1:2, cex=0.8,
title="Line types", text.font=4, bg='lightblue')
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type = "l",
col = "blue",
xlab = "Time (sampled)",
ylab = "Sentiment"
)
lines(OGLList$x[oglSample], OGLList$z[oglSample], col = "red")
lines(UOLList$x[uolSample], UOLList$z[uolSample], col = "black")
legend(0, 1,
legend=c("BBC", "oglobo", "uol"),
col=c("blue", "red", "black"), lty = 1:2, cex = 0.8,
title="Line types", text.font = 4, bg = 'lightblue')
legend(0, 1,
legend=c("BBC", "oglobo", "uol"),
col=c("blue", "red", "black"), lty = 1, cex = 0.8,
title="Line types", text.font = 4, bg = 'lightblue')
legend(0, 1,
legend=c("BBC", "oglobo", "uol"),
col=c("blue", "red", "black"), lty = 1, cex = 0.8,
text.font = 4, bg = 'lightblue')
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type = "l",
col = "blue",
xlab = "Time (sampled)",
ylab = "Sentiment"
)
lines(OGLList$x[oglSample], OGLList$z[oglSample], col = "red")
lines(UOLList$x[uolSample], UOLList$z[uolSample], col = "black")
legend(0, 1,
legend=c("BBC", "oglobo", "uol"),
col=c("blue", "red", "black"), lty = 1, cex = 0.8,
text.font = 4, bg = 'lightblue')
plot(bbcList$x[bbcSample],
bbcList$z[bbcSample],
type = "l",
col = "blue",
xlab = "Time (sampled)",
ylab = "Sentiment"
)
lines(OGLList$x[oglSample], OGLList$z[oglSample], col = "red")
lines(UOLList$x[uolSample], UOLList$z[uolSample], col = "black")
legend(0, 1,
legend=c("BBC", "oglobo", "uol"),
col=c("blue", "red", "black"), lty = 1, cex = 0.8,
text.font = 4, bg = 'lightblue')
# Correlation
c <- cor(cbind(bbcList$z[bbcSample], OGLList$z[oglSample], UOLList$z[uolSample]))
colnames(c) <- rownames(c) <- c("BBC", "oglobo", "uol")
corrplot::corrplot.mixed(c)
>>>>>>> d06a48078b62111a29031b0a4a9fd81ae19afa7a
